import streamlit as st
from dotenv import load_dotenv
import os
from src.database import SupabaseManager
from src.embeddings import EmbeddingGenerator
from src.llm import LLMManager
from typing import List, Dict
import logging
from pathlib import Path
import json
import csv
from io import StringIO

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
log_dir = Path(os.getenv("LOG_DIRECTORY", "logs"))
log_dir.mkdir(exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'app.log'),
        logging.StreamHandler()
    ]
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

def initialize_managers():
    """ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–"""
    return (
        SupabaseManager(),
        EmbeddingGenerator(),
        LLMManager()
    )

def search_and_generate_response(
    query: str,
    db_manager: SupabaseManager,
    embedding_generator: EmbeddingGenerator,
    llm_manager: LLMManager,
    summary_threshold: float,
    chunk_threshold: float
) -> tuple[str, List[Dict], List[Dict]]:
    """æ¤œç´¢ã¨å›ç­”ç”Ÿæˆã‚’å®Ÿè¡Œ"""
    try:
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
        query_embedding = embedding_generator.get_embedding(query)

        # ã‚µãƒãƒªãƒ¼æ¤œç´¢
        similar_summaries = db_manager.search_similar_summaries(
            query_embedding,
            threshold=summary_threshold
        )

        # ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢ï¼ˆã‚µãƒãƒªãƒ¼æ¤œç´¢ã®çµæœã‚’åˆ©ç”¨ï¼‰
        similar_chunks = db_manager.search_similar_chunks(
            query_embedding,
            guideline_matches=similar_summaries,
            threshold=chunk_threshold
        )
        
        # é–¾å€¤ã‚’è¶…ãˆã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_chunks = [chunk for chunk in similar_chunks if chunk['similarity'] >= chunk_threshold]

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        metadata = []
        guideline_titles = {}
        for summary in similar_summaries:
            meta = db_manager.get_guideline_metadata(
                summary['id'],
                similarity=summary['similarity']
            )
            if meta:
                metadata.append(meta)
                guideline_titles[summary['id']] = meta['title']

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆ
        response = llm_manager.generate_response(query, filtered_chunks, metadata)

        return response, metadata, filtered_chunks, guideline_titles

    except Exception as e:
        logging.error(f"Error in search_and_generate_response: {str(e)}")
        raise e

def main():
    st.title("GDPR Guidelines Search")
    
    # ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
    db_manager, embedding_generator, llm_manager = initialize_managers()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š
    with st.sidebar:
        st.header("Search Settings")
        
        # é¡ä¼¼åº¦ã®é–¾å€¤è¨­å®š
        summary_threshold = st.slider(
            "Summary Similarity Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.6,
            step=0.05,
            help="Minimum similarity score for guideline summaries"
        )
        
        chunk_threshold = st.slider(
            "Content Similarity Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.6,
            step=0.05,
            help="Minimum similarity score for content chunks"
        )
        
        # è¨­å®šã‚’é©ç”¨ã™ã‚‹ãƒœã‚¿ãƒ³
        st.markdown("---")
        update_settings = st.button(
            "ğŸ“ Update Search Configuration",
            help="Apply these threshold settings to your next search"
        )

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã®æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    query = st.text_input("ğŸ” Enter your question about GDPR guidelines")
    search_button = st.button("ğŸ” Search", disabled=not query)

    # æ¤œç´¢å®Ÿè¡Œã®æ¡ä»¶
    should_search = (
        search_button and 
        query and 
        (update_settings or 'last_settings' in st.session_state)
    )

    # è¨­å®šãŒæ–°ã•ã‚ŒãŸå ´åˆã€ã‚»ãƒƒã‚·ãƒ§ï¿½ï¿½çŠ¶æ…‹ã«ä¿å­˜
    if update_settings:
        st.session_state.last_settings = {
            'summary_threshold': summary_threshold,
            'chunk_threshold': chunk_threshold
        }
        st.success("Search configuration updated! You can now perform your search.")

    # æ¤œç´¢å®Ÿè¡Œ
    if should_search:
        try:
            with st.spinner('Searching and generating response...'):
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰æœ€å¾Œã®è¨­å®šã‚’ä½¿ç”¨
                settings = st.session_state.last_settings
                response, metadata, context_chunks, guideline_titles = search_and_generate_response(
                    query,
                    db_manager,
                    embedding_generator,
                    llm_manager,
                    settings['summary_threshold'],
                    settings['chunk_threshold']
                )

                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
                st.session_state.chat_history.append({
                    "query": query,
                    "response": response,
                    "metadata": metadata,
                    "chunks": context_chunks,
                    "guideline_titles": guideline_titles
                })

        except Exception as e:
            st.error(f"An error occurred: {str(e)}")
            return

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤ºï¼ˆé€†é †ï¼‰
    for chat_index, item in enumerate(reversed(st.session_state.chat_history)):
        st.write("---")
        
        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ã‚’å³å¯„ã›ã§é…ç½®
        col1, col2 = st.columns([6, 1])
        with col1:
            st.write("ğŸ¤” **Question:**")
        with col2:
            # ã“ã®ãƒãƒ£ãƒƒãƒˆã‚’CSVã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            output = StringIO()
            writer = csv.writer(output)
            writer.writerow(['Type', 'Content'])
            writer.writerow(['Question', item["query"]])
            writer.writerow(['Answer', item["response"]])
            writer.writerow([''])  # ç©ºè¡Œ
            
            # å‚ç…§ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æƒ…å ±
            writer.writerow(['Referenced Guidelines:'])
            for summary in item["metadata"]:
                writer.writerow([
                    'Guideline',
                    f'{summary["title"]} (Version: {summary["version"]}, Adopted: {summary["adopted_date"]})'
                ])
            
            # é–¢é€£ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            writer.writerow([''])
            writer.writerow(['Relevant Sections:'])
            sorted_chunks = sorted(item["chunks"], key=lambda x: x['similarity'], reverse=True)
            for chunk in sorted_chunks[:5]:  # ä¸Šä½5ä»¶
                if chunk['similarity'] >= chunk_threshold:
                    writer.writerow([
                        f'Similarity: {chunk["similarity"]:.2f}',
                        chunk['content'].replace('\n', ' ')  # æ”¹è¡Œã‚’ç©ºç™½ã«ç½®æ›
                    ])
            
            csv_data = output.getvalue()
            st.download_button(
                label="ğŸ“¥",
                data=csv_data,
                file_name=f"chat_export_{chat_index}.csv",
                mime="text/csv",
                help="Export this Q&A as CSV"
            )

        st.write(item["query"])
        st.write("ğŸ¤– **Answer:**")
        st.write(item["response"])

        # å‚è€ƒè³‡æ–™ã®è¡¨ç¤º
        st.write("ğŸ“š **Referenced Guidelines**")
        for summary in item["metadata"]:
            with st.expander(f"**{summary['title']}** (Similarity: {summary['similarity']:.2f})"):
                st.write(f"- Version: {summary['version']}")
                st.write(f"- Adopted: {summary['adopted_date']}")
                st.write(f"- Type: {summary['document_type']}")
                st.write("#### Summary")
                st.write(summary['summary'])

        # é–¢é€£ãƒãƒ£ãƒ³ã‚¯ï¼ˆé¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
        st.write("ğŸ“„ **Relevant Sections**")
        sorted_chunks = sorted(item["chunks"], key=lambda x: x['similarity'], reverse=True)
        guideline_titles = item.get("guideline_titles", {})

        # ä¸Šä½5å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’è¡¨ç¤º
        for i, chunk in enumerate(sorted_chunks[:5], 1):
            if chunk['similarity'] < chunk_threshold:
                continue
            
            guideline_title = guideline_titles.get(chunk['guideline_id'], "Unknown Guideline")
            
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ©ãƒ³ã‚¯ä»˜ãï¿½ï¿½ï¿½è¡¨ç¤º
            st.markdown(f"""
            <div style='
                border-left: 3px solid #e6e6e6;
                padding-left: 15px;
                margin-bottom: 5px;
            '>
                <div style='color: #666; margin-bottom: 10px;'>
                    <strong>#{i}</strong> | <strong>From: {guideline_title}</strong> (Similarity: {chunk['similarity']:.2f})
                </div>
                <p style='font-size: 1em; margin: 0 0 5px 0;'>{chunk['content']}</p>
            </div>
            """, unsafe_allow_html=True)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒœã‚¿ãƒ³ã‚’ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«è¦–è¦šçš„ã«é–¢é€£ä»˜ã‘ã‚‹
            st.markdown("""
            <div style='
                margin-left: 18px;
                margin-bottom: 20px;
                border-left: 3px solid #e6e6e6;
            '>
            """, unsafe_allow_html=True)
            
            with st.expander("ğŸ“– Show Full Context"):
                context_chunks = chunk.get('context_chunks', [])
                if not context_chunks:
                    st.write("No additional context available")
                    continue
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’é †åºä»˜ã‘ã¦å‡¦ç†
                sorted_contexts = sorted(context_chunks, key=lambda x: x['chunk_id'])
                combined_text = []
                
                for i, ctx in enumerate(sorted_contexts):
                    current_text = ctx['content']
                    
                    if ctx['chunk_id'] == chunk['id']:
                        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ãã®ã¾ã¾è¡¨ç¤º
                        combined_text.append(f"<p style='font-size: 1em; margin: 10px 0;'>{current_text}</p>")
                    else:
                        # å‰ã®ãƒãƒ£ãƒ³ã‚¯ã¨ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’ãƒã‚§ãƒƒã‚¯
                        if i > 0:
                            prev_text = sorted_contexts[i-1]['content']
                            overlap = find_overlap(prev_text, current_text)
                            if overlap:
                                current_text = current_text[len(overlap):]
                        
                        # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã¨ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’ãƒã‚§ãƒƒã‚¯
                        if i < len(sorted_contexts) - 1:
                            next_text = sorted_contexts[i+1]['content']
                            overlap = find_overlap(current_text, next_text)
                            if overlap:
                                current_text = current_text[:-len(overlap)]
                        
                        combined_text.append(f"<p style='font-size: 1em; margin: 10px 0;'>{current_text}</p>")
                
                st.markdown("".join(combined_text), unsafe_allow_html=True)
            
            st.markdown("</div>", unsafe_allow_html=True)

def find_overlap(text1: str, text2: str, min_length: int = 50) -> str:
    """2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆé–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’æ¤œå‡º"""
    # text1ã®æœ«å°¾ã¨text2ã®å…ˆé ­ã§æœ€é•·ã®å…±é€šéƒ¨åˆ†ã‚’æ¢ã™
    for length in range(min(len(text1), len(text2)), min_length - 1, -1):
        if text1[-length:] == text2[:length]:
            return text2[:length]
    return ""

if __name__ == "__main__":
    main()